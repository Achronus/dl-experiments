{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandits\n",
    "\n",
    "Bandit algorithms are one of the simplest RL algorithms, but highlight fundamental principles that make up most of the state-of-the-art ones today.\n",
    "\n",
    "In this notebook we will explore some simple variants:\n",
    "\n",
    "1. Firstly, we'll start by exploring how to do *trial and error learning* by taking random actions\n",
    "2. We'll build on it and introduce a mechanism for helping our agents to learn to make better decisions\n",
    "3. \n",
    "\n",
    "\n",
    "## Acting Randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine our agent is playing a single slot machine (bandit) with 6 arms. Each arm provides a different output, some more positive than others. We want our agent to be able to predict which arm is best to pull to maximise it's cumulative reward.\n",
    "\n",
    "We'll do this with a simple *action-value function* for a single state that depends on the next action. We'll assume we know the reward provided by that action too. Mathematically, we can express this as:\n",
    "\n",
    "$$\n",
    "V(a) = V(a) + \\alpha(r - V(a))\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $V(a)$: the value for a given action\n",
    "- $a$: action\n",
    "- $\\alpha$: the learning rate\n",
    "- $r$: reward\n",
    "\n",
    "Let's configure our hyperparameters with a fixed set of rewards $\\mathcal{R}$ with a reward for each arm, a small learning rate, 100 episodes, and configure our value function $V$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARDS = [1., .5, .2, .5, .6, .1, -.5]\n",
    "ARMS = len(REWARDS)\n",
    "EPISODES = 100\n",
    "LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Value = [.0] * ARMS\n",
    "Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Let's now code our *action-value* function, taking in the required components. This is will provide us with a prediction for a single actions reward value. We can use this to update our value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_value(action: int, V: list[float], R: list[float], lr: float) -> float:\n",
    "    return V[action] + lr * (R[action] - V[action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a learn method where the agent pulls a random arm and updates the value function. We'll also train it for 100 episodes and see how it does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(episodes: int, arms: int, V: list[float], R: list[float], lr: float) -> None:\n",
    "    for i in range(0, episodes):\n",
    "        action = random.randint(0, arms - 1)\n",
    "        V[action] = action_value(action, V, R, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn(EPISODES, ARMS, V=Value, R=REWARDS, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.68618940391,\n",
       " 0.38561603772519504,\n",
       " 0.15882177358107022,\n",
       " 0.41661409150166717,\n",
       " 0.4627392452702341,\n",
       " 0.08332281830033345,\n",
       " -0.3587852317595]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is looking pretty good! Remember that our reward values are:\n",
    "\n",
    "$$\n",
    "\\mathcal{R} = [1, 0.5, 0.2, 0.5, 0.6, 0.1, -0.5]\n",
    "$$\n",
    "\n",
    "With a bit more training we'd have very accurate results for each arm, where the first one provides the highest reward and last being the lowest.\n",
    "\n",
    "Now let's introduce a *policy* - a decision making strategy - for our agent. We'll keep it simple and make it act greedily by taking (exploiting) the best action based on our value function.\n",
    "\n",
    "## Greedy Policy\n",
    "\n",
    "$$\n",
    "a = \\max V\n",
    "$$\n",
    "\n",
    "We'll train it again like before with 100 episodes and see what new updated value function looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Value = [.0] * ARMS\n",
    "Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy(V: list[float]) -> int:\n",
    "    return V.index(max(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(episodes: int, V: list[float], R: list[float], lr: float) -> None:\n",
    "    for i in range(0, episodes):\n",
    "        action = greedy(V)\n",
    "        V[action] = action_value(action, V, R, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn(EPISODES, V=Value, R=REWARDS, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9999734386011123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that's interesting! Notice how the agent only uses the first arm? We'll, that's because it no longer has any exploration. We are only exploiting the first option available. The agent no longer has the ability to explore the full extent of its state space. \n",
    "\n",
    "We could expand this to multiple agents (bandits), but we can save that for more algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
